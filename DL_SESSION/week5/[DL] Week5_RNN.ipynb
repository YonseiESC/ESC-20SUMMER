{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2020 SUMMER ESC :: Week 5 RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. RNN (Recurrenct Neural Networks) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 RNN의 개념\n",
    "\n",
    "\n",
    "---\n",
    "**Recurrent: 되풀이되는, 반복되는**  \n",
    "\n",
    "RNN은 은닉층이 방향을 가진 엣지로 연결돼 <U>순환구조를 이루는 인공신경망</U>의 한 종류입니다. 순환구조로 인해 자료의 순서를 고려하여 학습할 수 있어 음성, 문자, 시계열 등 순차적으로 등장하는 데이터<U>(sequence data) 처리에 적합한 모델</U>입니다.\n",
    "\n",
    "---\n",
    "\n",
    "**rnn은 다른 신경망들과 어떻게 다른가?**\n",
    "\n",
    "일반적인 신경망에서는 데이터를 입력하면 입력층에서 은닉층까지 연산이 차근차근 진행되고 출력이 나가게 됩니다. 이 과정에서 입력 데이터는 모든 노드를 딱 한 번씩만 지나가게 됩니다. 데이터가 노드를 한 번만 지나가게 된다는 것은 데이터의 순서 즉 시간적인 측면을 고려하지 않는 구조라는 의미입니다. 데이터들의 시간 순서를 무시하고 현재 주어진 데이터를 통해서 독립적으로 학습을 합니다. \n",
    "\n",
    "하지만 RNN의 경우는 은닉층의 결과가 다시 같은 은닉층의 입력으로 들어가도록 연결되어 있습니다. 이런 특성이 RNN이 순서 또는 시간이라는 측면을 고려할 수 있는 특징을 가져다주게 됩니다. \n",
    "\n",
    "쉽에 말해, RNN은 우리가 어떤 글을 읽고 이해할 때에 전체 문장을 구성하는 단어들을 한번에 보고 문맥을 이해하는 것이 아닌, 문장을 구성하는 단어의 순서와 문맥을 이해면서 내용을 이해하는 것처럼 작동합니다.  \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**rnn의 활용**\n",
    "![](./img/Q8zv6TQ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 RNN의 구조\n",
    "---\n",
    "\n",
    "![](https://t2.daumcdn.net/thumb/R720x0/?fname=http://t1.daumcdn.net/brunch/service/user/17Xk/image/CaG8aFbUMK9sF6ce-JQewa125sE.png)\n",
    "RNN은 모든 cell이 전부 parameter를 공유합니다. 즉, 데이터는 sequence 순서에 따라 입력되나, 실제로 각 sequence를 계산하는 cell이 하나입니다. 따라서 RNN 모델은 좌측과 같이 fold된 상태로 표현할 수 있습니다.  \n",
    "\n",
    "<img src=\"https://i.imgur.com/s8nYcww.png\" width=\"80%\">\n",
    "위 그림은 기본적인 RNN의 구조입니다.</b>\n",
    "현재상태의 아웃풋 $y_{t}$는 $h_{t}$를 전달받아 갱신되는 구조입니다. 수식에서도 알 수 있듯이 hidden state를 계산할 때에는 tanh() 함수를 활성화 함수로 사용합니다.\n",
    "\n",
    "---\n",
    "**hidden state**   \n",
    "\n",
    "연산 결과가 출력되지 않고 다음 cell로 넘어가는 값을 hidden state라고 합니다. rnn 밖에서 보게되면 보이지 않는 값이기 때문에 hidden state라고 불립니다. <U>현재상태의 hidden state $h_{t}$는 직전 시점의 hidden state인 $h_{t-1}$를 받아 갱신됩니다.</U> 이 구조를 통해 이전 입력값의 처리 결과를 반영하게 되어 RNN 모델이 데이터의 순서를 이해할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 RNN의 순전파\n",
    "![](./img/rnn_fp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 RNN의 역전파\n",
    "**Back-Propagation Through Time(BPTT)**  \n",
    "\n",
    "RNN 또한 딥러닝 모델이기 때문에 gradient descent(경사하강법)을 이용하여 모델을 학습합니다. 하지만 기존 NN 모델과 달리 sequence에 의한 순환구조를 가지고 있기 때문에 RNN의 역전파 과정을 **BPTT(Back-Propagation Through Time)** 라고 부릅니다.  \n",
    "BPTT를 사용하는 이유는 각 파라미터들이 네트워크의 매 시간 스텝마다 공유되기 때문입니다.  \n",
    "<U>즉, t 시점에서의 parameter들의 업데이트는 시점 t 이후의 결과에도 영향을 미치게 됩니다.</U>  \n",
    "따라서,RNN의 Gradient 계산은 현재 시간의 단계에서만 계산하는 것이 아니라 이전 시간의 단계 또한 고려해야 합니다.  \n",
    "\n",
    "![](./img/rnn_bp.png)\n",
    "\n",
    "기본 RNN 모델에서 학습 parameter는 $W_{xh}$, $W_{hh}$, $W_{hy}$ 입니다.  \n",
    "$dW_{hy}$는 흘러들어온 그래디언트 $dy_{t}$에 로컬 그래디언트 ht를 곱해 구합니다. $dh_{t}$는 흘러들어온 그래디언트 $dy_{t}$에 $W_{hy}$를 곱한 값입니다. $dh_{raw}$는 흘러들어온 그래디언트인 $dh_{t}$에 로컬그래디언트인 $1-tanh^2(h_{raw})$을 곱해서 구합니다. 나머지도 동일한 방식으로 구합니다.\n",
    "\n",
    "![](./img/rnn_bp2.png)\n",
    "\n",
    "다만 위의 그림에 주의할 필요가 있습니다. RNN은 히든 노드가 순환 구조를 띄는 신경망입니다. 즉, $h_{t}$를 만들 때 $h_{t-1}$이 반영됩니다. 바꿔 말하면 아래 그림의 $dh_{t-1}$은 t-1시점의 Loss에서 흘러들어온 그래디언트 뿐만 아니라 * 에 해당되는 그래디언트 또한 더해져 동시에 반영된다는 뜻입니다.  \n",
    "따라서 $W_{xh}$, $W_{hh}$ 학습을 위한 그래디언트를 계산할 때에는 시점에 유의해주어야합니다.(BPTT라고 불리는 이유)\n",
    "\n",
    "조금 더 직관적인 이해를 원하시면 아래 링크를 참고해 주세요!  \n",
    "<https://aikorea.org/blog/rnn-tutorial-3/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 RNN의 장기의존성(Long-Term Dependency) 문제\n",
    "---\n",
    "\n",
    "<img src=\"https://mblogthumb-phinf.pstatic.net/MjAxODA3MDJfMTA3/MDAxNTMwNTM3NDA5MzA4.XVdGLg1QMZm-7GbqMi-qzp1kJ3HTL0xQIJYFkGuGDY8g.AEf4MVfIILOCyFG9mdSisodq6NkltDIxG3moV34azRsg.PNG.magnking/image.png?type=w800\" width=\"70%\">\n",
    "<img src=\"https://mblogthumb-phinf.pstatic.net/MjAxODA3MDJfMTMg/MDAxNTMwNTM3NDM1MTcx.q8hgizrupUf8TNvE9kx8y4lU3fBtvZTdApDdv3TuHw4g.gHiZh9DYIge6ICj0mT23RfmdmuubK7ISf8jNCvs4AvQg.PNG.magnking/image.png?type=w800\" width=\"70%\">\n",
    "\n",
    "입력 데이터의 sequence가 짧다면 RNN은 과거의 정보를 잘 이용하여 학습할 수 있습니다. 하지만 sequence가 길고, 필요한 정보의 시점과 현재 시점 간의 차이가 큰 경우 RNN의 성능이 떨어지는 문제가 발생합니다. 이를 RNN의 **장기 의존성문제**라고 합니다.   \n",
    "다시 말해, 처음 입력으로 받은 정보는 학습 초기에는 학습에 강한 영향을 끼치다가 더 많은 새로운 입력이 들어오면 처음 입력값의 영향력은 점차 감소하고 결국 학습에 아무런 영향을 끼치지 못하게 됩니다(Gradient Vanishing)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. LSTMs (Long Short Term Memory networks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 LSTM의 개념\n",
    "\n",
    "___\n",
    "\n",
    "**등장 배경**\n",
    "\n",
    "Long Short Term Memory networks (LSTMs)는 표준 RNN의 long-term dependencies, 즉 vanishing gradient 문제를 해결하기 위해 고안되었습니다. \n",
    "\n",
    "___\n",
    "\n",
    "**표준 RNN vs. LSTM**\n",
    "\n",
    "모든 RNN은 신경망의 반복되는 모듈들의 체인의 형태를 가지고 있습니다. 표준 RNN과 LSTM은 이 <U>반복되는 모듈이 가지고 있는 신경망 레이어의 개수</U>에서 차이가 있습니다.\n",
    "\n",
    "<U>표준 RNN에서의 반복되는 모듈은 한개의 신경망 레이어를 가지고 있습니다.</U> 예를 들어, 아래와 같이 표준 RNN에서의 반복되는 모듈은 하나의 하이퍼볼릭탄젠트 레이어를 가지고 있습니다.\n",
    "\n",
    "![](./img/LSTM3-SimpleRNN.png)\n",
    "\n",
    "<U>LSTM에서의 반복되는 모듈은 네개의 신경망 레이어를 가지고 있습니다.</U> 그리고 이들은 특별한 방식으로 상호작용 합니다.\n",
    "\n",
    "![](./img/LSTM3-chain.png)\n",
    "\n",
    "<U>Notation 참고</U>\n",
    "\n",
    "![](./img/LSTM2-notation.png)\n",
    "\n",
    "- 노란색 박스 : 신경망 레이어\n",
    "- 분홍색 원 : pointwise operation\n",
    "- 선 : 한 노드의 출력에서 다른 노드의 입력까지 전체 벡터를 전달\n",
    "- 선이 합쳐지는 것 : 내용이 결합\n",
    "- 선이 갈라지는 것 : 내용이 복사되어 복사본이 다른 위치로 이동\n",
    "\n",
    "___\n",
    "\n",
    "**Cell state**\n",
    "\n",
    "LSTM의 핵심은 <U>cell state</U>입니다. \n",
    "\n",
    "![](./img/LSTM3-C-line.png)\n",
    "\n",
    "다이어그램 상단의 수평선이 cell state를 나타냅니다. cell state는 약간의 사소한 선형 상호작용만을 가지며 전체 체인을 따라 직진합니다. 이를 통해 정보가 변하지 않은 채 그대로 흘러갈 수 있습니다. LSTM은 cell state에 정보를 추가하거나 제거할 수 있는 능력을 가지고 있습니다. 이를 가능하게 하는 구조가 <U>게이트</U>입니다. \n",
    "\n",
    "![](./img/LSTM3-gate.png)\n",
    "\n",
    "게이트는 선택적으로 정보를 통과시키는 방법입니다. 게이트는 시그모이드 신경망 레이어와 pointwise multiplication operation으로 구성되어 있습니다. 시그모이드 신경망 레이어는 0과 1 사이의 숫자를 출력하는데, 이는 각 구성요소가 얼마나 통과되어야 하는지를 나타냅니다. 0은 \"아무것도 통과시키지 말 것\"를 의미하고, 1은 \"모든 것을 통과시킬 것\"을 의미합니다. <U>LSTM은 cell state를 보호하고 통제하기 위해 이러한 게이트를 3개 가지고 있습니다.</U>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 LSTM의 구조\n",
    "___\n",
    "\n",
    "**Step 1. forget gate layer**\n",
    "\n",
    "![](./img/LSTM3-focus-f.png)\n",
    "\n",
    "첫번째 단계는 cell state에 어떠한 정보를 제거할 것인지를 결정하는 단계입니다. 이러한 결정은 forget gate layer라고 불리는 시그모이드 레이어를 통해 이루어집니다. 시그모이드 레이어는 $h_{t-1}$와 $x_t$을 보고, cell state $C_{t-1}$의 각각의 번호에 대하여 0과 1 사이의 숫자를 출력합니다. 1은 \"완전히 보관\"을 나타내는 한편 0은 \"완전히 제거\"를 나타내는 의미를 담고 있습니다.\n",
    "\n",
    "이전의 모든 단어를 바탕으로 다음 단어를 예측하려는 언어 모델을 예로 생각해봅시다. 여기에서, cell state는 정확한 대명사가 사용될 수 있도록 현재 주어의 성별을 포함하고자 합니다. 만약 새로운 주어를 보게 되면, 이전의 주어의 성별을 제거하고자 할 것입니다.\n",
    "\n",
    "___\n",
    "\n",
    "**Step 2. Input gate**\n",
    "\n",
    "![](./img/LSTM3-focus-i.png)\n",
    "\n",
    "다음 단계는 cell state에 어떠한 새로운 정보를 추가할 것인지를 결정하는 단계입니다.\n",
    " \n",
    "1. 시그모이드 레이어가 어떠한 값을 업데이트 할 것인지를 결정 : input gate layer\n",
    "2. 하이퍼볼릭탄젠트 레이어가 cell state에 추가될 수도 있는 새로운 후보 값들의 벡터인 $\\tilde{C_t}$를 생성 : update gate layer\n",
    "3. cell state를 업데이트 하기 위해 위 두가지를 결합\n",
    "\n",
    "언어 모델의 예에서, 이전 단계에서 제거하기로 결정한 이전의 주어의 성별을 대체하기 위해 새로운 주어의 성별을 cell state에 추가하기를 원합니다. \n",
    "\n",
    "___\n",
    "\n",
    "**Step 3. Cell state**\n",
    "\n",
    "![](./img/LSTM3-focus-c.png)\n",
    "\n",
    "이제 이전의 cell state인 $C_{t-1}$를 새로운 cell state인 $C_t$로 업데이트 하는 단계입니다. Step 1과 Step 2에서는 무엇을 해야 할지 결정하였다면, Step 3에서는 이를 실제로 수행합니다.\n",
    "\n",
    "먼저 이전의 cell state인 $C_{t-1}$에 $f_t$를 곱합니다. 이는 이전 단계에서 제거하기로 결정하였던 것들을 제거하기 위한 작업입니다. 그러고 나서 여기에 $i_t * \\tilde{C_t}$를 더합니다. 이는 각 state의 값을 업데이트 하기로 결정한 정도에 따라 스케일링 하는 작업입니다. 이들이 새로운 후보 값들이 됩니다.\n",
    "\n",
    "언어 모델의 예에서, 이전 단계에서 결정한 바를 바탕으로, 이 단계에서는 실제로 이전의 주어의 성별에 대한 정보를 제거하고 새로운 정보를 추가합니다.\n",
    "\n",
    "___\n",
    "\n",
    "**Step 4. Output gate**\n",
    "\n",
    "![](./img/LSTM3-focus-o.png)\n",
    "\n",
    "마지막 단계는 무엇을 출력할 것인지를 결정하는 단계입니다. 이때의 출력물은 cell state를 기반으로 하되 이를 필터링 한 버전이 됩니다. 먼저 cell state의 어떤 부분을 출력할 것인지를 결정하는 시그모이드 레이어를 실행하여 $o_t$를 얻습니다. 그리고 cell state $C_t$를 하이퍼볼릭탄젠트 함수에 통과시킵니다. 그러고 나서 출력하기로 결정한 부분만 출력할 수 있도록 이들을 즉 $o_t$와 $\\tanh \\left( C_t \\right)$를 곱합니다. \n",
    "\n",
    "언어 모델의 예에서, 방금 주어를 봤기 때문에 다음에 다가올 경우에 대비하여 동사와 관련된 정보를 출력하고 싶을 것입니다. 예를 들어, 다음에 어떤 형태로 동사가 결합되어야 하는지 알 수 있도록 주어가 단수인지 복수인지를 출력할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 LSTM의 순전파 (Forward pass)\n",
    "___\n",
    "\n",
    "![](./img/7Jk6szL.png)\n",
    "\n",
    "여기서 주목해야 할 점은 $H_t$입니다. 이 행렬 $H_t$를 행을 기준으로 4등분 하여 $f$, $i$, $g$, $o$ 각각에 해당하는 활성함수를 적용함으로써 $f_t$, $i_t$, $g_t$, $o_t$를 계산합니다. $f$, $i$, $o$의 활성함수는 시그모이드이고, $g$의 활성함수는 하이퍼볼릭탄젠트입니다. 이를 그림으로 나타내면 다음과 같습니다.\n",
    "\n",
    "![](./img/73zzDsC.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 LSTM의 역전파 (Backward pass)\n",
    "\n",
    "___\n",
    "\n",
    "LSTM의 backward pass의 전체적인 과정은 다음과 같습니다.\n",
    "\n",
    "![](./img/2BZtc2l.gif)\n",
    "\n",
    "우선 $df_t$, $di_t$, $dg_t$, $do_t$를 구하기 전까지의 backward pass는 아래와 같습니다. 표준 RNN과 유사한 것을 볼 수 있습니다.\n",
    "\n",
    "![](./img/hTPFF4A.png)\n",
    "\n",
    "다음으로 $df_t$, $di_t$, $dg_t$, $do_t$를 구하는 과정에 대해 살펴보겠습니다. $f$, $i$, $o$의 활성함수는 시그모이드이고 $g$의 활성함수는 하이퍼볼릭탄젠트입니다. 아래와 같이 각각의 활성함수에 대한 로컬 그래디언트를 구해 흘러들어온 그래디언트를 곱해주면 $df_t$, $di_t$, $dg_t$, $do_t$를 구할 수 있습니다. \n",
    "\n",
    "![](./img/screenshot.png)\n",
    "\n",
    "forward pass에서 $H_t$를 4등분 하여 $f_t$, $i_t$, $g_t$, $o_t$를 구했던 것처럼, backward pass에서는 이렇게 구한 $df_t$, $di_t$, $dg_t$, $do_t$를 다시 합쳐 $dH_t$를 만듭니다.\n",
    "\n",
    "이렇게 구한 $dH_t$는 아래와 같이 표준 RNN과 같은 방식으로 backward pass가 이루어지는 구조입니다.\n",
    "\n",
    "![](./img/sM4rlmr.png)\n",
    "\n",
    "LSTM은 cell state와 hidden state가 재귀적으로 구해지는 네트워크입니다. 따라서 cell state의 그래디언트와 hidden state의 그래디언트는 직전 시점의 그래디언트 값에 영향을 받습니다. 이는 표준 RNN과 마찬가지입니다. 이를 backward pass 시 반영해야 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 참고자료\n",
    "<https://ratsgo.github.io/natural%20language%20processing/2017/03/09/rnnlstm/>  \n",
    "<https://aikorea.org/blog/rnn-tutorial-3/>  \n",
    "<http://colah.github.io/posts/2015-08-Understanding-LSTMs/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. RNN Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"i like dog\", \"i love coffee\", \"i hate milk\", \"you like cat\", \"you love milk\", \"you hate coffee\"]\n",
    "dtype = torch.float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word Processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = list(set(\" \".join(sentences).split())) # 중복되는 단어들을 제거하여 리스트 생성\n",
    "word_dict = {w: i for i, w in enumerate(word_list)} # word를 key로, index를 value로 갖는 딕셔너리 생성\n",
    "number_dict = {i: w for i, w in enumerate(word_list)} # index를 key로, word를 value로 갖는 딕셔너리 생성\n",
    "n_class = len(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'milk',\n",
       " 1: 'you',\n",
       " 2: 'dog',\n",
       " 3: 'like',\n",
       " 4: 'coffee',\n",
       " 5: 'hate',\n",
       " 6: 'i',\n",
       " 7: 'love',\n",
       " 8: 'cat'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TextRNN Parameter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = len(sentences)\n",
    "n_step = 2  # 학습 하려고 하는 문장의 길이 - 1\n",
    "n_hidden = 5  # 은닉층 사이즈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch(sentences):\n",
    "    input_batch = []\n",
    "    target_batch = []\n",
    "    \n",
    "    for sen in sentences:\n",
    "        word = sen.split()\n",
    "        input = [word_dict[n] for n in word[:-1]]\n",
    "        target = word_dict[word[-1]]\n",
    "        input_batch.append(np.eye(n_class)[input])  # One-Hot Encoding\n",
    "        target_batch.append(target)\n",
    "    \n",
    "    return input_batch, target_batch\n",
    "\n",
    "input_batch, target_batch = make_batch(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0.]]),\n",
       " array([[0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 1., 0.]]),\n",
       " array([[0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0., 0.]]),\n",
       " array([[0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0.]]),\n",
       " array([[0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 1., 0.]]),\n",
       " array([[0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0., 0.]])]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_batch = torch.tensor(input_batch, dtype=torch.float32, requires_grad=True)\n",
    "target_batch = torch.tensor(target_batch, dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TextRNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextRNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TextRNN, self).__init__()\n",
    "        \n",
    "        self.rnn = nn.RNN(input_size=n_class, hidden_size=n_hidden, dropout=0.3)\n",
    "        self.W = nn.Parameter(torch.randn([n_hidden, n_class]).type(dtype))\n",
    "        self.b = nn.Parameter(torch.randn([n_class]).type(dtype))\n",
    "        self.Softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, hidden, X):\n",
    "        X = X.transpose(0, 1)\n",
    "        outputs, hidden = self.rnn(X, hidden)\n",
    "        outputs = outputs[-1]  # 최종 예측 Hidden Layer\n",
    "        model = torch.mm(outputs, self.W) + self.b  # 최종 예측 최종 출력 층\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\njj06\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "model = TextRNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss & Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0100 cost = 0.574982\n",
      "Epoch: 0200 cost = 0.127370\n",
      "Epoch: 0300 cost = 0.047592\n",
      "Epoch: 0400 cost = 0.025828\n",
      "Epoch: 0500 cost = 0.016676\n",
      "[['i', 'like'], ['i', 'love'], ['i', 'hate'], ['you', 'like'], ['you', 'love'], ['you', 'hate']] -> ['dog', 'coffee', 'milk', 'cat', 'milk', 'coffee']\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "for epoch in range(500):\n",
    "    hidden = torch.zeros(1, batch_size, n_hidden, requires_grad=True)\n",
    "    output = model(hidden, input_batch)\n",
    "    loss = criterion(output, target_batch)\n",
    "    \n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n",
    "        \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "input = [sen.split()[:2] for sen in sentences]\n",
    "\n",
    "hidden = torch.zeros(1, batch_size, n_hidden, requires_grad=True)\n",
    "predict = model(hidden, input_batch).data.max(1, keepdim=True)[1]\n",
    "print([sen.split()[:2] for sen in sentences], '->', [number_dict[n.item()] for n in predict.squeeze()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. LSTM Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"i like dog\", \"i love coffee\", \"i hate milk\", \"you like cat\", \"you love milk\", \"you hate coffee\"]\n",
    "dtype = torch.float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word Processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = list(set(\" \".join(sentences).split()))\n",
    "word_dict = {w: i for i, w in enumerate(word_list)}\n",
    "number_dict = {i: w for i, w in enumerate(word_list)}\n",
    "n_class = len(word_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TextRNN Parameter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = len(sentences)\n",
    "n_step = 2  # 학습 하려고 하는 문장의 길이 - 1\n",
    "n_hidden = 5  # 은닉층 사이즈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch(sentences):\n",
    "    input_batch = []\n",
    "    target_batch = []\n",
    "    \n",
    "    for sen in sentences:\n",
    "        word = sen.split()\n",
    "        input = [word_dict[n] for n in word[:-1]]\n",
    "        target = word_dict[word[-1]]\n",
    "        \n",
    "        input_batch.append(np.eye(n_class)[input])  # One-Hot Encoding\n",
    "        target_batch.append(target)\n",
    "    \n",
    "    return input_batch, target_batch\n",
    "\n",
    "input_batch, target_batch = make_batch(sentences)\n",
    "input_batch = torch.tensor(input_batch, dtype=torch.float32, requires_grad=True)\n",
    "target_batch = torch.tensor(target_batch, dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TextLSTM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TextLSTM, self).__init__()\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=n_class, hidden_size=n_hidden, dropout=0.3)\n",
    "        self.W = nn.Parameter(torch.randn([n_hidden, n_class]).type(dtype))\n",
    "        self.b = nn.Parameter(torch.randn([n_class]).type(dtype))\n",
    "        self.Softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, hidden_and_cell, X):\n",
    "        X = X.transpose(0, 1)\n",
    "        outputs, hidden = self.lstm(X, hidden_and_cell)\n",
    "        outputs = outputs[-1]  # 최종 예측 Hidden Layer\n",
    "        model = torch.mm(outputs, self.W) + self.b  # 최종 예측 최종 출력 층\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Traning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TextLSTM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss & Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0100 cost = 0.346767\n",
      "Epoch: 0200 cost = 0.032417\n",
      "Epoch: 0300 cost = 0.014241\n",
      "Epoch: 0400 cost = 0.008376\n",
      "Epoch: 0500 cost = 0.005602\n",
      "[['i', 'like'], ['i', 'love'], ['i', 'hate'], ['you', 'like'], ['you', 'love'], ['you', 'hate']] -> ['dog', 'coffee', 'milk', 'cat', 'milk', 'coffee']\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "for epoch in range(500):\n",
    "    hidden = torch.zeros(1, batch_size, n_hidden, requires_grad=True)\n",
    "    cell = torch.zeros(1, batch_size, n_hidden, requires_grad=True)\n",
    "    output = model((hidden, cell), input_batch)\n",
    "    loss = criterion(output, target_batch)\n",
    "    \n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n",
    "        \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "input = [sen.split()[:2] for sen in sentences]\n",
    "\n",
    "hidden = torch.zeros(1, batch_size, n_hidden, requires_grad=True)\n",
    "cell = torch.zeros(1, batch_size, n_hidden, requires_grad=True)\n",
    "predict = model((hidden, cell), input_batch).data.max(1, keepdim=True)[1]\n",
    "print([sen.split()[:2] for sen in sentences], '->', [number_dict[n.item()] for n in predict.squeeze()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 과제\n",
    "\n",
    "- 과제1) 모두의딥러닝 시즌2 강의의 longsequence 코드 lstm 모델 적용해보기.  \n",
    "- 과제2) <https://tutorials.pytorch.kr/intermediate/char_rnn_classification_tutorial.html>  \n",
    "위 링크 따라해보기!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
