{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN, LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 모두의딥러닝 시즌2 강의의 longsequence 코드 lstm 모델 적용해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x12f1e5d55d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random seed to make results deterministic and reproducible\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = (\"if you want to build a ship, don't drum up people together to \"\n",
    "            \"collect wood and don't assign them tasks and work, but rather \"\n",
    "            \"teach them to long for the endless immensity of the sea.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dictionary\n",
    "char_set = list(set(sentence))\n",
    "char_dic = {c: i for i, c in enumerate(char_set)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper parameters\n",
    "dic_size = len(char_dic)\n",
    "hidden_size = len(char_dic)\n",
    "sequence_length = 10  # Any arbitrary number\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 if you wan -> f you want\n",
      "1 f you want ->  you want \n",
      "2  you want  -> you want t\n",
      "3 you want t -> ou want to\n",
      "4 ou want to -> u want to \n",
      "5 u want to  ->  want to b\n",
      "6  want to b -> want to bu\n",
      "7 want to bu -> ant to bui\n",
      "8 ant to bui -> nt to buil\n",
      "9 nt to buil -> t to build\n",
      "10 t to build ->  to build \n",
      "11  to build  -> to build a\n",
      "12 to build a -> o build a \n",
      "13 o build a  ->  build a s\n",
      "14  build a s -> build a sh\n",
      "15 build a sh -> uild a shi\n",
      "16 uild a shi -> ild a ship\n",
      "17 ild a ship -> ld a ship,\n",
      "18 ld a ship, -> d a ship, \n",
      "19 d a ship,  ->  a ship, d\n",
      "20  a ship, d -> a ship, do\n",
      "21 a ship, do ->  ship, don\n",
      "22  ship, don -> ship, don'\n",
      "23 ship, don' -> hip, don't\n",
      "24 hip, don't -> ip, don't \n",
      "25 ip, don't  -> p, don't d\n",
      "26 p, don't d -> , don't dr\n",
      "27 , don't dr ->  don't dru\n",
      "28  don't dru -> don't drum\n",
      "29 don't drum -> on't drum \n",
      "30 on't drum  -> n't drum u\n",
      "31 n't drum u -> 't drum up\n",
      "32 't drum up -> t drum up \n",
      "33 t drum up  ->  drum up p\n",
      "34  drum up p -> drum up pe\n",
      "35 drum up pe -> rum up peo\n",
      "36 rum up peo -> um up peop\n",
      "37 um up peop -> m up peopl\n",
      "38 m up peopl ->  up people\n",
      "39  up people -> up people \n",
      "40 up people  -> p people t\n",
      "41 p people t ->  people to\n",
      "42  people to -> people tog\n",
      "43 people tog -> eople toge\n",
      "44 eople toge -> ople toget\n",
      "45 ople toget -> ple togeth\n",
      "46 ple togeth -> le togethe\n",
      "47 le togethe -> e together\n",
      "48 e together ->  together \n",
      "49  together  -> together t\n",
      "50 together t -> ogether to\n",
      "51 ogether to -> gether to \n",
      "52 gether to  -> ether to c\n",
      "53 ether to c -> ther to co\n",
      "54 ther to co -> her to col\n",
      "55 her to col -> er to coll\n",
      "56 er to coll -> r to colle\n",
      "57 r to colle ->  to collec\n",
      "58  to collec -> to collect\n",
      "59 to collect -> o collect \n",
      "60 o collect  ->  collect w\n",
      "61  collect w -> collect wo\n",
      "62 collect wo -> ollect woo\n",
      "63 ollect woo -> llect wood\n",
      "64 llect wood -> lect wood \n",
      "65 lect wood  -> ect wood a\n",
      "66 ect wood a -> ct wood an\n",
      "67 ct wood an -> t wood and\n",
      "68 t wood and ->  wood and \n",
      "69  wood and  -> wood and d\n",
      "70 wood and d -> ood and do\n",
      "71 ood and do -> od and don\n",
      "72 od and don -> d and don'\n",
      "73 d and don' ->  and don't\n",
      "74  and don't -> and don't \n",
      "75 and don't  -> nd don't a\n",
      "76 nd don't a -> d don't as\n",
      "77 d don't as ->  don't ass\n",
      "78  don't ass -> don't assi\n",
      "79 don't assi -> on't assig\n",
      "80 on't assig -> n't assign\n",
      "81 n't assign -> 't assign \n",
      "82 't assign  -> t assign t\n",
      "83 t assign t ->  assign th\n",
      "84  assign th -> assign the\n",
      "85 assign the -> ssign them\n",
      "86 ssign them -> sign them \n",
      "87 sign them  -> ign them t\n",
      "88 ign them t -> gn them ta\n",
      "89 gn them ta -> n them tas\n",
      "90 n them tas ->  them task\n",
      "91  them task -> them tasks\n",
      "92 them tasks -> hem tasks \n",
      "93 hem tasks  -> em tasks a\n",
      "94 em tasks a -> m tasks an\n",
      "95 m tasks an ->  tasks and\n",
      "96  tasks and -> tasks and \n",
      "97 tasks and  -> asks and w\n",
      "98 asks and w -> sks and wo\n",
      "99 sks and wo -> ks and wor\n",
      "100 ks and wor -> s and work\n",
      "101 s and work ->  and work,\n",
      "102  and work, -> and work, \n",
      "103 and work,  -> nd work, b\n",
      "104 nd work, b -> d work, bu\n",
      "105 d work, bu ->  work, but\n",
      "106  work, but -> work, but \n",
      "107 work, but  -> ork, but r\n",
      "108 ork, but r -> rk, but ra\n",
      "109 rk, but ra -> k, but rat\n",
      "110 k, but rat -> , but rath\n",
      "111 , but rath ->  but rathe\n",
      "112  but rathe -> but rather\n",
      "113 but rather -> ut rather \n",
      "114 ut rather  -> t rather t\n",
      "115 t rather t ->  rather te\n",
      "116  rather te -> rather tea\n",
      "117 rather tea -> ather teac\n",
      "118 ather teac -> ther teach\n",
      "119 ther teach -> her teach \n",
      "120 her teach  -> er teach t\n",
      "121 er teach t -> r teach th\n",
      "122 r teach th ->  teach the\n",
      "123  teach the -> teach them\n",
      "124 teach them -> each them \n",
      "125 each them  -> ach them t\n",
      "126 ach them t -> ch them to\n",
      "127 ch them to -> h them to \n",
      "128 h them to  ->  them to l\n",
      "129  them to l -> them to lo\n",
      "130 them to lo -> hem to lon\n",
      "131 hem to lon -> em to long\n",
      "132 em to long -> m to long \n",
      "133 m to long  ->  to long f\n",
      "134  to long f -> to long fo\n",
      "135 to long fo -> o long for\n",
      "136 o long for ->  long for \n",
      "137  long for  -> long for t\n",
      "138 long for t -> ong for th\n",
      "139 ong for th -> ng for the\n",
      "140 ng for the -> g for the \n",
      "141 g for the  ->  for the e\n",
      "142  for the e -> for the en\n",
      "143 for the en -> or the end\n",
      "144 or the end -> r the endl\n",
      "145 r the endl ->  the endle\n",
      "146  the endle -> the endles\n",
      "147 the endles -> he endless\n",
      "148 he endless -> e endless \n",
      "149 e endless  ->  endless i\n",
      "150  endless i -> endless im\n",
      "151 endless im -> ndless imm\n",
      "152 ndless imm -> dless imme\n",
      "153 dless imme -> less immen\n",
      "154 less immen -> ess immens\n",
      "155 ess immens -> ss immensi\n",
      "156 ss immensi -> s immensit\n",
      "157 s immensit ->  immensity\n",
      "158  immensity -> immensity \n",
      "159 immensity  -> mmensity o\n",
      "160 mmensity o -> mensity of\n",
      "161 mensity of -> ensity of \n",
      "162 ensity of  -> nsity of t\n",
      "163 nsity of t -> sity of th\n",
      "164 sity of th -> ity of the\n",
      "165 ity of the -> ty of the \n",
      "166 ty of the  -> y of the s\n",
      "167 y of the s ->  of the se\n",
      "168  of the se -> of the sea\n",
      "169 of the sea -> f the sea.\n"
     ]
    }
   ],
   "source": [
    "# data setting\n",
    "x_data = []\n",
    "y_data = []\n",
    "\n",
    "for i in range(0, len(sentence) - sequence_length):\n",
    "    x_str = sentence[i:i + sequence_length]\n",
    "    y_str = sentence[i + 1: i + sequence_length + 1]\n",
    "    print(i, x_str, '->', y_str)\n",
    "\n",
    "    x_data.append([char_dic[c] for c in x_str])  # x str to index\n",
    "    y_data.append([char_dic[c] for c in y_str])  # y str to index\n",
    "\n",
    "x_one_hot = [np.eye(dic_size)[x] for x in x_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform as torch tensor variable\n",
    "X = torch.FloatTensor(x_one_hot)\n",
    "Y = torch.LongTensor(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare RNN + FC\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layers):\n",
    "        super(Net, self).__init__()\n",
    "        self.lstm = torch.nn.LSTM(input_dim, hidden_dim, num_layers=layers, batch_first=True)\n",
    "        self.fc = torch.nn.Linear(hidden_dim, hidden_dim, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _status = self.lstm(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net(dic_size, hidden_size, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss & optimizer setting\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
      "                                                                                                                                                                                   \n",
      "  ddd            ddddd d d  dd d        d              dd                   d     d   ddd  dd     dd    d                        d              ddd   ddd    ddd   ddd   d    ddd  \n",
      "  o                                                                                                                                                                                \n",
      "                                                                                                                                                                                   \n",
      "                                                                                                                                                                                   \n",
      "                                                                                                       t                                                                    t      \n",
      "  t    t  t t     t tt  t              t   t   t  t   t   tt tt tt    t   tt         t      t   ttt    t   t          tt   tt t   t   tt tt   t   tt  tt  t       t   t    tt  t  t\n",
      "       o    o        o       o          o  o   o  o  oo    o  o  o    o   oo   o     o   o  o    o     o   o          oo   oo     o    o  o       oo   o   o          o     o      \n",
      "      oo   oo lo   ooo       o  o       o lo   o  o loo   oo lo lo    o looo   o     o   o  o   oo     o   o lo lo    oo   oo  o  o    o lo lo   loo   o  oo      o   o     o      \n",
      "  oo   oo   o lo   oooo      o  o  o    o loo loo o looo  oo lo looo  o l oo   o  o  o   o  oo   o     o   o lo lo    ooo  oo  o  oo   o lo l  o looo  o  oo      o   o     oo     \n",
      "  oo   o  o o lo    ooo   l  o  o  o   lo loo  o oo   oo  oo  o looo  o   oo   o  o  o      oo  oo     o   o lo lo   oooo  oo  o  oo  oo lo    o looo  o   o      o   o    ooo o   \n",
      "  oo   o    o l   t  o       o    lo    o lo   o  o l oo  oo lo lo   lo l oo   o     o      oo  oo     o   o le l     oo   oo  o loo  oo lo l  o looo  o lo       o   o     oo     \n",
      "  oh   o    h l   t toh      o    to    e lee  e th   oh  th  o lo t lo   to   o    to     thh   h    to   o         eehs  th  o loo  th lo    o lohh  e  e       o   t t  thh     \n",
      " toh   o   th t   t thhs    te  t te t le leh  e th   hhs th th le h  o   to  te  t ths    thh  th    to  th     te tethh  thsth  oh  th th   to  ehh te  t     tth   e h  ohh t   \n",
      " tee   odd to t   t ttts  t te  t tt t le lee le th  tohr th to le t lo   ted te  t tt   t thh  to d  ted to  t ltt ttthe  tod h loo  to to l to leoh te le     ttt   e t  ehh te l\n",
      " toe  todd to t   t ttts  t do  t tddt le lee le to  toor to to le t lo   ttd to  t tts  t toer to d  ttd to  t ltt tttoer tod o loor to lo l to  toerte le   d ttt   t t  toerte l\n",
      " toes tost to t   t tstss t dod t tttt do loo lo to  toes to to lost tod  tes tod t tts io toos toss itts tod t ltt tttoes tosto toos to to   tod toostes tss dtttts  t ts toosts  \n",
      " toer tont to b   t tnts  t bo  t tn t bo loo lo to  ooer to to lont bo   tos to  t tns io toor to   ites to  t ltt botoer tonto toor to lo l ton toorto  o s ittto   o t  toerts  \n",
      " toer tont to b   t tnts  t ton t t to l  boo lo to tooer to to lont to l tos to  t tns io toor to s itns to  t l t tntoer tonto toer to bo ' ton toerte  tnssi ttesiio tn toerts  \n",
      " toer tont to b   t tnts  t ton't tn t bn boo lo to  toer to to lont ton  tnd don't tnsiit toer to isitnd don t b t dntoer tonco toer to bo ' ton toorce  tnsii toesiio tn toercs t\n",
      " toer tont to b ilt tntn lt ton't tn t bp boo lo to ctoer to to lont ton' tnd don't tnsign toer to is tnd don t b t dotoer tonco toer to bo ' ton toeree  tnssdmteesido tn toerte t\n",
      " toer tont to b  lt tntssim ton't tndm bl loe lo to  toer to bo lo t ton' tnd don't tnsigm toer to is tnd donlt b t tnthes tonth toer to bo ' tonltoerces tsssit tnsigy tn toercss \n",
      " toer tont to b ilt tnths t ton't tncm lp uoe lo th eeher eh to lo t tork tnd ton't tnsign ther to is tnd ton o lut toeher threh eher to bo ' tor thereer tnsstntnesito or theree t\n",
      " toer tont do luild tntsslt don't dn m lp loo le to  ther to lo le t uon' tnd don't dnsitn ther wonis dnd donlt lut uomher tonth toer tonlo ' tor toerch lt sitmthedity or toerte b\n",
      " toer tont tonluild tntss t don't dn m lp uoo le ton ther tonto le t tond tnd don't tnsign ther tonis tnd don t lut damher toath ther tondon' tor toereh ltssitmthsdity hr therte t\n",
      " toer tont to uuild tnths t ton't tn m up uoe le to ether to tolle t toad tnd ton't tnsign ther tonis tnd ton t lut dather torth ther to uon' aor toereh  tsssimhemsity tr theree t\n",
      " toer tont to luild tsthslt don't drum lp leo le toleooer to tolle t woad tnd don't dnsign toer tonks tnd ton o lut datoer toreh toer tollo ' aor toereed tssiimmhedity tr toeree t\n",
      "  oer wont to luild astnslt don't drum pp leo le to etoem to colle t woad tnd don't dnsign toem tonks tnd don t lut datoer toa h toem to lon' aor toemeed tssiimmnnditn or toereesi\n",
      "  oer tont to luild astnst, don't drum up peo le tonetoer to bolle t woad tnd don't dssign them tonks and wonkt lut datoer tonch toer tonleng aor toereed tssiimmessitn or toereest\n",
      "  hor tost to build astnst, don't drum up peo le to etoem to bolle t doad tnd won't dssign them tonks and wonk, but dather tonch toe  tonbeng aor toe endlessiimmendita or toeree t\n",
      "  hor tost to build astnsp, don't drum up ueo le th etoem to collect aord and won't dssign them tonks and wonk, but dather tosch the  to bon' aor toe endless imeensity or thereh t\n",
      "  hor wost th build astnsp, don't drum up people th etoer to collect aord and won't dssign ther tosks and wonk, but dather thsch ther to beng aor themendless immensity or therehsp\n",
      "u tor bost th build a tnil, don't drum up peo le to ethem to collect aord and won't dssign ther tosks and wonk, but dather thsch ther to beng aor toemendless immensity or theree l\n",
      "u tor wost th build a snip, don't arum up people to ethem to collect aoad and won't dssign ther tosks and work, but dather thnch them to bong aor themendless immensity or theree l\n",
      "u tor wont th build a snip, don't drum up people to ethem th collect doad and don't dssign ther tosks and work, but dathen thnch ther to bong for the endless immensity or therce t\n",
      "u hou wont th build a ship, don't drum up people to ethem th collect boad and don't dssign them tosks and work, but dather thach the  to bong for the endless immensity or the ce t\n",
      "u oou wont th build a ship, don't arum up people to ethem th collect boad and don't assign them tosks and work, but dather thach them to long for the endless immensity or themee t\n",
      "u oou wont th luild a ship, don't arum up people together to collect doad and don't assign them tosks and work, but dather toach them to long for the endless immensity or themcest\n",
      "u oou wont th build a ship, don't arum up people to ether to collect woad and don't dssign them tosks and work, but rather toach them to long for the endless immensity or the eest\n",
      "u oou wont to build a ship, don't drum up people to ether eo collect woad and don't dssign ther tasks and work, but rather eorch them ta long for the cndless immensity or thercesd\n",
      "u oou wont to build a ship, don't drum up people to ether to collect woad and won't dssign them tasks and work, but rather toach them ta long for the endless immensity or the eedt\n",
      "u oou wont to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather toach them ta long for the endless immensity or the eeat\n",
      "u oou wont to luild a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather toach them ta long for the endless immensity or the ceal\n",
      "u oou wont to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather toach them ta long for the endless immensity of the eeal\n",
      "u oou wont to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather toach them ta long for the endless immensity of the eeal\n",
      "l you wont to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather toach them ta long for the endless immensity of the eeat\n",
      "l you wont to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather toach them to long for the endless immensity of the ceaa\n",
      "l you wont to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather toach them to long for the endless immensity of the seag\n",
      "l you wont to build a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather toach them to long for the endless immensity of the seal\n",
      "l rou want to build a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather teach them to long for the endless immensity of the seag\n",
      "l rou want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the seag\n",
      "g you want to build a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather teach them to long for the endless immensity of the seag\n",
      "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seag\n",
      "g you want to build a ship, don't drum up people together te collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seag\n",
      "g you want to build a ship, don't drum up people together te collect wood and don't dssign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "g you want to build a ship, don't arum up people together te collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "g you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "g you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "f you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "f you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "f you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "f you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "f you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "f you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "f you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "f you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "f you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "f you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "f you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "f you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n"
     ]
    }
   ],
   "source": [
    "# start training\n",
    "for i in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = net(X)\n",
    "    loss = criterion(outputs.view(-1, dic_size), Y.view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    results = outputs.argmax(dim=2)\n",
    "    predict_str = \"\"\n",
    "    for j, result in enumerate(results):\n",
    "        # print(i, j, ''.join([char_set[t] for t in result]), loss.item())\n",
    "        if j == 0:\n",
    "            predict_str += ''.join([char_set[t] for t in result])\n",
    "        else:\n",
    "            predict_str += char_set[result[-1]]\n",
    "\n",
    "    print(predict_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 링크 따라하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://tutorials.pytorch.kr/intermediate/char_rnn_classification_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data/names\\\\Arabic.txt', 'data/names\\\\Chinese.txt', 'data/names\\\\Czech.txt', 'data/names\\\\Dutch.txt', 'data/names\\\\English.txt', 'data/names\\\\French.txt', 'data/names\\\\German.txt', 'data/names\\\\Greek.txt', 'data/names\\\\Irish.txt', 'data/names\\\\Italian.txt', 'data/names\\\\Japanese.txt', 'data/names\\\\Korean.txt', 'data/names\\\\Polish.txt', 'data/names\\\\Portuguese.txt', 'data/names\\\\Russian.txt', 'data/names\\\\Scottish.txt', 'data/names\\\\Spanish.txt', 'data/names\\\\Vietnamese.txt']\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def findFiles(path): return glob.glob(path)\n",
    "\n",
    "print(findFiles('data/names/*.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import string\n",
    "\n",
    "all_letters = string.ascii_letters + \" .,;'\"\n",
    "n_letters = len(all_letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slusarski\n"
     ]
    }
   ],
   "source": [
    "# 유니코드 문자열을 ASCII로 변환, https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters)\n",
    "\n",
    "print(unicodeToAscii('Ślusàrski'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 언어의 이름 목록인 category_lines 사전 생성\n",
    "category_lines = {}\n",
    "all_categories = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLines(filename):\n",
    "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "    return [unicodeToAscii(line) for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in findFiles('data/names/*.txt'):\n",
    "    category = os.path.splitext(os.path.basename(filename))[0]\n",
    "    all_categories.append(category)\n",
    "    lines = readLines(filename)\n",
    "    category_lines[category] = lines\n",
    "\n",
    "n_categories = len(all_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Abandonato', 'Abatangelo', 'Abatantuono', 'Abate', 'Abategiovanni']\n"
     ]
    }
   ],
   "source": [
    "print(category_lines['Italian'][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import torch\n",
    "# all_letters 로 문자의 주소 찾기, 예시 \"a\" = 0\n",
    "def letterToIndex(letter):\n",
    "    return all_letters.find(letter)\n",
    "\n",
    "# 검증을 위해서 한개의 문자를 <1 x n_letters> Tensor로 변환\n",
    "def letterToTensor(letter):\n",
    "    tensor = torch.zeros(1, n_letters)\n",
    "    tensor[0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "# 한 줄(이름)을  <line_length x 1 x n_letters>,\n",
    "# 또는 One-Hot 문자 벡터의 Array로 변경\n",
    "def lineToTensor(line):\n",
    "    tensor = torch.zeros(len(line), 1, n_letters)\n",
    "    for li, letter in enumerate(line):\n",
    "        tensor[li][0][letterToIndex(letter)] = 1\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0.]])\n",
      "torch.Size([5, 1, 57])\n"
     ]
    }
   ],
   "source": [
    "print(letterToTensor('J'))\n",
    "\n",
    "print(lineToTensor('Jones').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.i2o(combined)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden = 128\n",
    "rnn = RNN(n_letters, n_hidden, n_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = letterToTensor('A')\n",
    "hidden =torch.zeros(1, n_hidden)\n",
    "\n",
    "output, next_hidden = rnn(input, hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.9141, -2.8279, -2.9665, -2.8490, -2.7858, -2.8627, -2.8609, -2.9190,\n",
      "         -2.9679, -2.7830, -2.8715, -2.9210, -2.9142, -2.8214, -2.9451, -2.9629,\n",
      "         -2.9222, -2.9638]], grad_fn=<LogSoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "input = lineToTensor('Albert')\n",
    "hidden = torch.zeros(1, n_hidden)\n",
    "\n",
    "output, next_hidden = rnn(input[0], hidden)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Italian', 9)\n"
     ]
    }
   ],
   "source": [
    "def categoryFromOutput(output):\n",
    "    top_n, top_i = output.topk(1) # 텐서의 가장 큰 값 및 주소\n",
    "    category_i = top_i[0].item()     # 텐서에서 정수 값으로 변경\n",
    "    return all_categories[category_i], category_i\n",
    "\n",
    "print(categoryFromOutput(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category = Arabic / line = Asker\n",
      "category = Spanish / line = Zambrano\n",
      "category = Irish / line = Ceallach\n",
      "category = Arabic / line = Baba\n",
      "category = Italian / line = Abelli\n",
      "category = Scottish / line = Gordon\n",
      "category = Japanese / line = Asari\n",
      "category = Arabic / line = Toma\n",
      "category = Chinese / line = Lau\n",
      "category = Vietnamese / line = Ma\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def randomChoice(l):\n",
    "    return l[random.randint(0, len(l) - 1)]\n",
    "\n",
    "def randomTrainingExample():\n",
    "    category = randomChoice(all_categories)\n",
    "    line = randomChoice(category_lines[category])\n",
    "    category_tensor = torch.tensor([all_categories.index(category)], dtype=torch.long)\n",
    "    line_tensor = lineToTensor(line)\n",
    "    return category, line, category_tensor, line_tensor\n",
    "\n",
    "for i in range(10):\n",
    "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
    "    print('category =', category, '/ line =', line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.005 # 이것을 너무 높게 설정하면 발산할 수 있고, 너무 낮으면 학습이 되지 않을 수 있습니다.\n",
    "\n",
    "def train(category_tensor, line_tensor):\n",
    "    hidden = rnn.initHidden()\n",
    "\n",
    "    rnn.zero_grad()\n",
    "\n",
    "    for i in range(line_tensor.size()[0]):\n",
    "        output, hidden = rnn(line_tensor[i], hidden)\n",
    "\n",
    "    loss = criterion(output, category_tensor)\n",
    "    loss.backward()\n",
    "\n",
    "    # 매개변수의 경사도에 학습률을 곱해서 그 매개변수의 값에 더합니다.\n",
    "    for p in rnn.parameters():\n",
    "        p.data.add_(p.grad.data, alpha=-learning_rate)\n",
    "\n",
    "    return output, loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "n_iters = 100000\n",
    "print_every = 5000\n",
    "plot_every = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 도식화를 위한 손실 추적\n",
    "current_loss = 0\n",
    "all_losses = []\n",
    "\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 5% (0m 59s) 2.5992 Hew / Chinese ✓\n",
      "10000 10% (1m 47s) 2.6482 Felix / Portuguese ✗ (French)\n",
      "15000 15% (2m 21s) 1.1302 Oh  / Korean ✓\n",
      "20000 20% (3m 1s) 2.1268 Campos / Greek ✗ (Portuguese)\n",
      "25000 25% (3m 41s) 0.4847 Rinaldi / Italian ✓\n",
      "30000 30% (4m 19s) 1.6224 Kozlow / Greek ✗ (Polish)\n",
      "35000 35% (5m 16s) 2.4555 Quaranta / Spanish ✗ (Italian)\n",
      "40000 40% (5m 46s) 0.1839 Stawski / Polish ✓\n",
      "45000 45% (6m 14s) 0.4898 Najjar / Arabic ✓\n",
      "50000 50% (6m 40s) 0.1488 Agelakos / Greek ✓\n",
      "55000 55% (7m 21s) 0.9844 Huffmann / German ✓\n",
      "60000 60% (7m 54s) 1.3469 Simpson / Scottish ✓\n",
      "65000 65% (8m 24s) 1.7146 Baumgartner / Russian ✗ (German)\n",
      "70000 70% (8m 50s) 1.8234 Warren / German ✗ (English)\n",
      "75000 75% (9m 17s) 3.5551 Salomon / French ✗ (Polish)\n",
      "80000 80% (9m 44s) 1.0735 Kingdon / Scottish ✗ (English)\n",
      "85000 85% (10m 10s) 0.8885 Amadori / Italian ✓\n",
      "90000 90% (10m 35s) 0.6262 Kattan / Arabic ✓\n",
      "95000 95% (11m 3s) 2.3548 Rompa / Czech ✗ (Dutch)\n",
      "100000 100% (11m 29s) 2.9256 Schenk / Korean ✗ (Czech)\n"
     ]
    }
   ],
   "source": [
    "for iter in range(1, n_iters + 1):\n",
    "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
    "    output, loss = train(category_tensor, line_tensor)\n",
    "    current_loss += loss\n",
    "\n",
    "    # iter 숫자, 손실, 이름, 추측 화면 출력\n",
    "    if iter % print_every == 0:\n",
    "        guess, guess_i = categoryFromOutput(output)\n",
    "        correct = '✓' if guess == category else '✗ (%s)' % category\n",
    "        print('%d %d%% (%s) %.4f %s / %s %s' % (iter, iter / n_iters * 100, timeSince(start), loss, line, guess, correct))\n",
    "\n",
    "    # 현재 평균 손실을 전체 손실 리스트에 추가\n",
    "    if iter % plot_every == 0:\n",
    "        all_losses.append(current_loss / plot_every)\n",
    "        current_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1a44bef8e80>]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(all_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 혼란 행렬에서 정확한 추측을 추적\n",
    "confusion = torch.zeros(n_categories, n_categories)\n",
    "n_confusion = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주어진 라인의 출력 반환\n",
    "def evaluate(line_tensor):\n",
    "    hidden = rnn.initHidden()\n",
    "\n",
    "    for i in range(line_tensor.size()[0]):\n",
    "        output, hidden = rnn(line_tensor[i], hidden)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예시들 중에 어떤 것이 정확하게 예측되었는지 기록\n",
    "for i in range(n_confusion):\n",
    "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
    "    output = evaluate(line_tensor)\n",
    "    guess, guess_i = categoryFromOutput(output)\n",
    "    category_i = all_categories.index(category)\n",
    "    confusion[category_i][guess_i] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 행을 합계로 나누어 정규화\n",
    "for i in range(n_categories):\n",
    "    confusion[i] = confusion[i] / confusion[i].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 도식 설정\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(confusion.numpy())\n",
    "fig.colorbar(cax)\n",
    "\n",
    "# 축 설정\n",
    "ax.set_xticklabels([''] + all_categories, rotation=90)\n",
    "ax.set_yticklabels([''] + all_categories)\n",
    "\n",
    "# 모든 tick에서 레이블 지정\n",
    "ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "# sphinx_gallery_thumbnail_number = 2\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "주축에서 벗어난 밝은 점을 선택하여 잘못 추측한 언어를 표시 할 수 있습니다. 예를 들어 한국어는 중국어로 이탈리아어로 스페인어로. 그리스어는 매우 잘되는 것으로 영어는 매우 나쁜것으로 보입니다. (다른 언어들과 중첩 때문으로 추정)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "> Dovesky\n",
      "(-0.39) Russian\n",
      "(-1.64) Czech\n",
      "(-3.02) English\n",
      "\n",
      "> Jackson\n",
      "(-0.15) Scottish\n",
      "(-2.91) English\n",
      "(-3.56) Russian\n",
      "\n",
      "> Satoshi\n",
      "(-1.06) Polish\n",
      "(-1.44) Japanese\n",
      "(-1.51) Italian\n"
     ]
    }
   ],
   "source": [
    "def predict(input_line, n_predictions=3):\n",
    "    print('\\n> %s' % input_line)\n",
    "    with torch.no_grad():\n",
    "        output = evaluate(lineToTensor(input_line))\n",
    "\n",
    "        # Get top N categories\n",
    "        topv, topi = output.topk(n_predictions, 1, True)\n",
    "        predictions = []\n",
    "\n",
    "        for i in range(n_predictions):\n",
    "            value = topv[0][i].item()\n",
    "            category_index = topi[0][i].item()\n",
    "            print('(%.2f) %s' % (value, all_categories[category_index]))\n",
    "            predictions.append([value, all_categories[category_index]])\n",
    "\n",
    "predict('Dovesky')\n",
    "predict('Jackson')\n",
    "predict('Satoshi')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
